# Token Minimizer üéì

![Token Minimizer ‚Äî Dark theme dual-panel interface for optimizing AI prompts](https://img.shields.io/badge/Stack-HTML%20%7C%20CSS%20%7C%20JavaScript%20ES%20Modules-58a6ff?style=for-the-badge)

## Introducci√≥n

Token Minimizer is a client-side web tool that **reduces AI prompt costs by 40-60%** by translating prompts into high-density Chinese (logograms) or stripping filler words. Students and developers who interact frequently with GPT-4 and other token-billed models can paste their prompts, press one button, and get back a compressed version that the AI understands perfectly ‚Äî at a fraction of the token cost. The entire pipeline runs in the browser with zero backend, leveraging ES module imports, free translation APIs, and the official GPT-4 tokenizer.

---

## Desarrollo de las partes

### 1. HTML Structure ‚Äî Semantic Dual-Panel Layout

The page is a single `index.html` (107 lines) organized as a vertical flex column inside `.app-container`. The header contains the üéì branding and a live tokenizer status indicator. A collapsible `<details>` mission-box educates users on why Chinese saves tokens. The main area splits into two symmetric panels ‚Äî input and output ‚Äî each with its own header, textarea, token counter, and action buttons.

```html
<section class="mission-box">
    <details>
        <summary>üí° <strong>How does this save money?</strong> (Click to learn)</summary>
        <div class="mission-content">
            <p>AI models (like GPT-4) charge by "tokens".</p>
            <ul>
                <li><strong>English/Spanish</strong> are inefficient (1 word ‚âà 1.3 tokens).</li>
                <li><strong>Chinese (Logograms)</strong> is dense (1 sentence ‚âà few tokens).</li>
            </ul>
        </div>
    </details>
</section>
```

The input panel offers a language selector (`auto/es/en/fr/pt`), a Nuclear Mode toggle, and the "MINIMIZE TOKENS ‚ûî" button. The output panel shows a savings badge, a read-only textarea, and Copy / Save `.toon` buttons. Open Graph meta tags and `aria-label` attributes improve discoverability and accessibility.

---

### 2. CSS Design System ‚Äî GitHub-Inspired Dark Theme

All colors are driven by CSS custom properties on `:root`, creating a cohesive dark palette:

```css
:root {
    --bg-dark: #0f1115;
    --panel-bg: #161b22;
    --border: #30363d;
    --primary: #58a6ff;
    --accent: #238636;
    --danger: #da3633;
    --font-mono: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace;
}
```

The two panels use `display: flex` with `flex: 1` for equal sizing, collapsing to column layout at 768 px via `@media`. The `.savings-badge` class changes dynamically: `.positive` adds a green tint for savings, `.negative` switches to red for cost increases. A custom toggle switch replaces the native checkbox for Nuclear Mode, using `::before` pseudo-element positioning and `translateX` for the sliding animation.

Accessibility additions include `*:focus-visible` with a `--primary` outline ring and `@media (prefers-reduced-motion: reduce)` to disable all transitions for users who request it.

---

### 3. Tiktoken Integration ‚Äî Accurate GPT-4 Token Counting

The application imports `encodingForModel` directly from a CDN as an ES module:

```javascript
import { encodingForModel } from "https://cdn.jsdelivr.net/npm/js-tiktoken@1.0.10/+esm";

let enc = null;
(async () => {
    try {
        enc = encodingForModel("gpt-4");
        els.status.textContent = "Logic Ready (Tiktoken Loaded)";
    } catch (e) {
        els.status.textContent = "Offline Mode (Estimates Only)";
    }
})();
```

The `count()` function uses the real tokenizer when available, falling back to `Math.ceil(text.length / 3)` for offline mode. This gives users real-time, accurate token counts as they type, just like the OpenAI tokenizer. The IIFE initialization runs at page load and updates the header status indicator between "Loading Logic..." ‚Üí "Logic Ready" ‚Üí or "Offline Mode."

---

### 4. SUFFIXES & STOPWORDS ‚Äî The Linguistic Strategy

Two key data structures power the optimization:

**SUFFIXES** ‚Äî ultra-short Chinese instructions appended after translation to tell the AI which language to reply in:

```javascript
const SUFFIXES = {
    'es': '\n(Áî®Ë•øËØ≠Á≠î)',   // "Reply in Spanish" ‚Äî just 4 characters
    'en': '\n(Áî®Ëã±ËØ≠Á≠î)',
    'fr': '\n(Áî®Ê≥ïËØ≠Á≠î)',
    'pt': '\n(Áî®Ëë°ËØ≠Á≠î)',
    'auto': '\n(Áî®ÂéüËØ≠Á≠î)'
};
```

**STOPWORDS** ‚Äî 60+ regex patterns covering articles, prepositions, pronouns, polite phrases, and be-verbs across English, Spanish, and French. These are the highest-frequency, lowest-meaning words that LLMs don't need to understand the prompt:

```javascript
const STOPWORDS = [
    /\bplease\b/gi, /\bcould you\b/gi, /\bkindly\b/gi,
    /\bpor favor\b/gi, /\bpodr√≠as\b/gi,
    /\bthe\b/gi, /\ba\b/gi, /\ban\b/gi,
    /\bel\b/gi, /\bla\b/gi, /\blos\b/gi, /\blas\b/gi,
    // ... 60+ patterns total
];
```

Every regex uses `\b` word boundaries to avoid breaking words like "theater" when stripping "the."

---

### 5. `compactChinese()` ‚Äî Post-Translation Compression

After translating to Chinese, this function squeezes out extra bytes:

```javascript
function compactChinese(text) {
    return text
        .replace(/([\u4e00-\u9fa5])\s+([\u4e00-\u9fa5])/g, '$1$2') // Remove spaces between Hanzi
        .replace(/\s+/g, ' ')
        .replace(/Ôºå/g, ',').replace(/„ÄÇ/g, '.')     // Full-width ‚Üí ASCII punctuation
        .replace(/Ôºö/g, ':').replace(/Ôºõ/g, ';')
        .replace(/Ôºü/g, '?').replace(/ÔºÅ/g, '!')
        .replace(/Ôºà/g, '(').replace(/Ôºâ/g, ')')
        .replace(/"/g, '"').replace(/"/g, '"');
}
```

The Unicode range `\u4e00-\u9fa5` targets CJK Unified Ideographs. Spaces between Chinese characters are artifacts from translation APIs and waste tokens. Converting full-width punctuation (Ôºå„ÄÇÔºö) to ASCII equivalents (,.:) saves 1-2 bytes each ‚Äî small per character but significant over a full prompt.

---

### 6. `translateChunked()` ‚Äî Dual-API Translation Pipeline

This function handles the actual translation via free APIs with graceful degradation:

```javascript
async function translateChunked(text, source, target) {
    const rawChunks = text.match(/[^.!?\n]+[.!?\n]+(?:\s|$)|[^.!?\n]+$/g) || [text];
    // Merge chunks into ‚â§450 char blocks
    // ...
    for (let c of chunks) {
        // 1. MyMemory API (primary)
        let url = `https://api.mymemory.translated.net/get?q=${encodeURIComponent(c)}&langpair=${source}|${target}`;
        // Retry on 429 rate limit
        // 2. Lingva via AllOrigins proxy (fallback)
        const targetApi = `https://lingva.ml/api/v1/${source}/${target}/${encodeURIComponent(c)}`;
        const proxyUrl = `https://api.allorigins.win/raw?url=${encodeURIComponent(targetApi)}`;
        // 3. Keep original if both fail
        await delay(500); // Politeness throttle
    }
}
```

The text is split at sentence boundaries (`.!?\n`) and merged into 450-character chunks to respect API limits. **MyMemory** is tried first with a single retry on HTTP 429. If it fails, **Lingva** is called through the **AllOrigins** CORS proxy so the app works from localhost without a backend. A 500ms politeness delay between requests avoids rate-limiting.

---

### 7. Three-Strategy Optimizer ‚Äî The Decision Engine

The core click handler on "MINIMIZE TOKENS" implements three distinct optimization paths:

| Condition | Strategy | What Happens |
|-----------|----------|--------------|
| Nuclear ON + Long text (‚â•150 chars) | **Chinese Translation** | Strip stopwords ‚Üí translate to `zh-CN` ‚Üí compact Chinese ‚Üí add reply suffix |
| Nuclear ON + Short text (<150 chars) | **Caveman Mode** | Strip stopwords only ‚Äî no API call. Fast and free. |
| Nuclear OFF | **English Translation** | Translate to English (most token-efficient Western language) |

```javascript
if (useNuclear) {
    targetLang = isShort ? 'caveman' : 'zh-CN';
} else {
    targetLang = 'en';
}
```

Caveman Mode is the clever optimization: for short prompts, the translation overhead (API latency + suffix tokens) would actually *increase* cost. So the app just strips filler words and returns instantly, showing a notification: "‚ö° Smart Bypass: Short prompt detected."

---

## Presentaci√≥n del proyecto

Token Minimizer opens with a clean, dark interface split into two symmetric panels ‚Äî write on the left, read the result on the right. The üéì icon and tagline "Save on AI costs. Invest in your brain" immediately communicate the tool's purpose.

Typing a prompt in the left panel shows a live token count updating in the footer. The language dropdown defaults to Spanish but supports auto-detection, English, French, and Portuguese. Nuclear Mode is toggled on by default for maximum savings.

Clicking "MINIMIZE TOKENS ‚ûî" (or pressing Ctrl+Enter) triggers the optimization pipeline. The button changes to "Processing..." while API calls execute. Within seconds, the right panel fills with the compressed text ‚Äî often dense Chinese characters when Nuclear Mode is active ‚Äî and the savings badge lights up green: "SAVED: 52% (84 Tok)" for a typical prompt.

The collapsible mission box explains the economics: English and Spanish encode at roughly 1.3 tokens per word, while Chinese logograms carry an entire sentence in a handful of tokens. Students can click to learn, then close the panel to focus on their work.

For short prompts under 150 characters, the app skips the API entirely and enters Caveman Mode ‚Äî stripping all filler words in under a millisecond and showing a notification explaining the smart bypass. The `.toon` file format lets users save their optimized prompts for later use with a single click.

---

## Conclusi√≥n

Token Minimizer demonstrates that a meaningful developer tool can be built entirely in the browser with zero infrastructure. By combining the GPT-4 tokenizer library, free translation APIs with automatic failover, and a linguistic strategy rooted in information density, the application delivers real cost savings ‚Äî often 40-60% per prompt ‚Äî without requiring any server or API keys. The three-strategy decision engine adapts intelligently to input length, and the dual-API pipeline with Lingva fallback ensures reliable operation even when primary services are rate-limited. It is a practical tool that solves a real problem for anyone who works with token-billed language models on a budget.
